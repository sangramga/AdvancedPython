
# coding: utf-8

## Import libraries and modules
import numpy as np
import pandas as pd
import pprint

from sklearn import pipeline
from sklearn import preprocessing
from sklearn import feature_selection
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.externals import joblib 

##Load the data.
data = pd.read_csv(filepath_or_buffer='./classify.csv', sep=' ', header=None)
#print data.head()
#print data.shape

## Drop the last column
data.drop(labels=data.columns[len(data.columns)-1], axis=1, inplace=True)
#print data.head()

## Split data into feature-matrix and the target-vector
y = data.iloc[:,0]
X = data.iloc[:, 1:len(data.columns)]

## Make pipeline using data preprocessing step, best k-features selection step and classifier algo step
pipe = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()), 
                         ('selectkbest', feature_selection.SelectKBest()),
                         ('randomforestclassifier', RandomForestClassifier(random_state=222))])
#print pipeline.get_params()

## Random Forest Classifier without prunning:
## Declare default hyperparameters to tune i.e. giving the default values as it is
defaulat_hyperparam = {'selectkbest__k':[17]} # selecting all the features in the default no-prun run

## Create model without pruning using cross-validation pipeline
no_prune_clf = GridSearchCV(estimator=pipe, param_grid=defaulat_hyperparam, scoring="f1", cv=10)
no_prune_clf.fit(X, y)

## Refit on the entire training set
# No additional code needed if no_prune_clf.refit == True (default is True)

## Feature-selection scores
features = no_prune_clf.best_estimator_.named_steps['selectkbest']
print "\n******** Random-Forest Classifier: No pruning: All features"
print "The feature selection scores(ANOVA F-value between label/feature) are as follows:"
print "Feature     Score "
for itr in range(X.shape[1]):
    print(str(itr+1) +"           "+str(features.scores_[itr]))

## F1 score for Classifier without pruning:
print "\n******** Random-Forest Classifier: No pruning: All features: 10-fold Cross-Validation results: "
pprint.pprint(no_prune_clf.cv_results_)

print "******** Random-Forest Classifier: No pruning: All features: F1-score of the best estimator: ", no_prune_clf.best_score_

##Save model for future use
joblib.dump(no_prune_clf, 'no_prune_clf.pkl')
# To load: no_prune_clf = joblib.load('no_prune_clf.pkl')

# Random Forest Classifier with prunning:
## Declare hyperparameters for prunning
pruning_hyperparam = { 'selectkbest__k':[12]
                       #randomforestclassifier__n_estimators' : [10]
                       #,randomforestclassifier__max_features' : ['sqrt']
                       ,'randomforestclassifier__min_samples_leaf' : [2]
                       ,'randomforestclassifier__min_samples_split' : [4]
                     }

## Create model with pruning using cross-validation pipeline
pruned_clf = GridSearchCV(estimator=pipe, param_grid=pruning_hyperparam, scoring="f1", cv=10)
pruned_clf.fit(X, y)

## F1 score for Classifier with pruning:
print "\n******** Random-Forest Classifier: With pruning : Pruning parameters are set as follows:"
print "Number of best features to be considered (out of 17): 12"
print "min_samples_leaf(The minimum number of samples required to be at a leaf node): 2 "
print "min_samples_split(The minimum number of samples required to split an internal node): 4 "

print "\n******** Random-Forest Classifier: With pruning : 10-fold Cross-Validation results: "
pprint.pprint(pruned_clf.cv_results_)

print "\n******** Random-Forest Classifier: With pruning : F1-score of the best estimator: ", pruned_clf.best_score_

#Save model for future use
joblib.dump(pruned_clf, 'pruned_clf.pkl');
# To load: pruned_clf = joblib.load('pruned_clf.pkl')

